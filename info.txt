The aim of this file is to tell you what is going on in here.
Directory houses stuff required to deal with Speaker Change Detection Problem

Description of which stuff is where:

The directory, wav contains:
The audio files for train,val and test purposes.
The driectory, vad contains:
The ground truth files for the TIMIT data set.

We also have to account for the number of various ways in which we can make speaker changed files.
It can be S1-S2, or S1-S2 overlap, or S1-S2-S1, or many other such ways.

Discuss with Sir, as to which all to take, and correspondingly either alter the directory structure, or make directories in the existing structure.

UPDATE[5/6/17]: Discussion with Sir
We won't be taking silence, and also we won't be taking overlapped speech, atleast right now.
Also, we are fixing some parameters which decide which label to give to a frame.
If the ratio of speaker 1 to speaker 2 in a frame is greater then, 0.1 then we label the frame as speaker change frame.
If the ratio is less than that, we label the frame as non speaker change frame
Also, if a frame has ratio greater than 0.1 of silence frames, we label it something else.

Labels:
0: Single speaker frame
1: Speaker change frame[With 10% condition]
2: Silence frame[With 15% condition]

The decision window was taken to be 400 ms, because it helped in getting required number of frames.
The number of frames available for classification is around 4%.
We still need to remove the class, with Label 2 as this data is not clean

Update[Done with feature generation,9/6/17]:

On some features to try out:
Gini index[Which kind of tries to measure the inharmonicity of the structure ]

Update[17/6/17]:
We will also need to analyze if male and female effects are effecting it, and also if the pitch estimates can help.
The data file has a very large size and also need to reduce that. Maybe reduce the data on the fly, or reduce the number of training, validation and test samples.

Update[21/6/17]:
For gammatone features, here is the workflow:
[read_gamma_feats.m]First for each file, find the gamamatone features. Store them in feats/gamma folder
[convert_to_htk.m]Next, for each instance in a file, find it context, and then store it in context, possibly with subfolder indicating how much context
[combine_gamma.py, run with combine.sh]Now, combine all the context, this is also where the filtering happens
Just input this to the model

Update[23/6/17]
feats is complete[both gamma and mfcc]
Complete the context generation
For combining the files, combine the files into smaller ones, say 12 L samples or so
